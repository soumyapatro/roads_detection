{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full explanation of the code, visit http://ataspinar.com/2017/12/04/using-convolutional-neural-networks-to-detect-features-in-sattelite-images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import requests\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy import ndimage\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#We are using owslib to download images from a WMS Service\n",
    "#install with 'pip install owslib'\n",
    "\n",
    "import owslib\n",
    "\n",
    "from owslib.wms import WebMapService\n",
    "\n",
    "#pyshp is necessary for loading and saving shapefiles\n",
    "#install with 'pip install pyshp'\n",
    "import shapefile\n",
    "\n",
    "# Install opencv with 'pip install opencv-python'\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_min = 90000\n",
    "y_min = 427000\n",
    "dx, dy = 200, 200\n",
    "no_tiles_x = 100\n",
    "no_tiles_y = 100\n",
    "total_no_tiles = no_tiles_x * no_tiles_y\n",
    "\n",
    "x_max = x_min + no_tiles_x * dx\n",
    "y_max = y_min + no_tiles_y * dy\n",
    "bounding_box = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "TILE_FOLDER = \"../datasets/image_tiles_200/\"\n",
    "URL_TILES = \"https://geodata.nationaalgeoregister.nl/luchtfoto/rgb/wms?request=GetCapabilities\"\n",
    "\n",
    "URL_SHP = 'https://www.rijkswaterstaat.nl/apps/geoservices/geodata/dmc/nwb-wegen/geogegevens/shapefile/Nederland_totaal/01-08-2017/Wegvakken/Wegvakken.shp'\n",
    "URL_PRF = 'https://www.rijkswaterstaat.nl/apps/geoservices/geodata/dmc/nwb-wegen/geogegevens/shapefile/Nederland_totaal/01-08-2017/Wegvakken/Wegvakken.prj'\n",
    "URL_DBF = 'https://www.rijkswaterstaat.nl/apps/geoservices/geodata/dmc/nwb-wegen/geogegevens/shapefile/Nederland_totaal/01-08-2017/Wegvakken/Wegvakken.dbf'\n",
    "URL_SHX = 'https://www.rijkswaterstaat.nl/apps/geoservices/geodata/dmc/nwb-wegen/geogegevens/shapefile/Nederland_totaal/01-08-2017/Wegvakken/Wegvakken.shx'\n",
    "\n",
    "URLS_SHAPEFILES = [URL_SHP, URL_PRF, URL_DBF, URL_SHX]\n",
    "\n",
    "DATA_FOLDER = \"../data/nwb_wegvakken/\"\n",
    "\n",
    "json_filename = DATA_FOLDER + '2017_09_wegvakken.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Downloading the image tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Skip if you already have the image tiles. Will take ~ 2hours.\n",
    "wms = WebMapService(URL_TILES, version='1.1.1')\n",
    "\n",
    "if not os.path.exists(TILE_FOLDER):\n",
    "    os.makedirs(TILE_FOLDER)\n",
    "\n",
    "for ii in range(0,no_tiles_x):\n",
    "    print(ii)\n",
    "    for jj in range(0,no_tiles_y):\n",
    "        ll_x_ = x_min + ii*dx\n",
    "        ll_y_ = y_min + jj*dy\n",
    "        bbox = (ll_x_, ll_y_, ll_x_ + dx, ll_y_ + dy) \n",
    "        img = wms.getmap(layers=['Actueel_ortho25'], srs='EPSG:28992', bbox=bbox, size=(256, 256), format='image/jpeg', transparent=True)\n",
    "        filename = \"{}{}_{}_{}_{}.jpg\".format(TILE_FOLDER, bbox[0], bbox[1], bbox[2], bbox[3])\n",
    "        out = open(filename, 'wb')\n",
    "        out.write(img.read())\n",
    "        out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Downloading the shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Skip if you already have the shapefiles. Will take ~ 1hour.\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.makedirs(DATA_FOLDER)\n",
    "\n",
    "for url in URLS_SHAPEFILES:\n",
    "    filename = url.split('/')[-1]\n",
    "    print(\"Downloading file {}\".format(filename))\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(DATA_FOLDER + filename, 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading shapefile and converting to (GEO)Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Skip if conversion already done and \"json_filename\" already saved\n",
    "def json_serial(obj):\n",
    "    \"\"\"JSON serializer for objects not serializable by default json code\"\"\"\n",
    "\n",
    "    if isinstance(obj, (datetime, date)):\n",
    "        serial = obj.isoformat()\n",
    "        return serial\n",
    "    if isinstance(obj, bytes):\n",
    "        return {'__class__': 'bytes',\n",
    "                '__value__': list(obj)}\n",
    "    raise TypeError (\"Type %s not serializable\" % type(obj))\n",
    "\n",
    "reader = shapefile.Reader(DATA_FOLDER + 'Wegvakken.shp')\n",
    "fields = reader.fields[1:]\n",
    "field_names = [field[0] for field in fields]\n",
    "\n",
    "buffer = []\n",
    "for sr in reader.shapeRecords()[:500000]:\n",
    "    atr = dict(zip(field_names, sr.record))\n",
    "    geom = sr.shape.__geo_interface__\n",
    "    buffer.append(dict(type=\"Feature\", geometry=geom, properties=atr)) \n",
    "\n",
    "\n",
    "json_file = open(json_filename, \"w\")\n",
    "json_file.write(json.dumps({\"type\": \"FeatureCollection\", \"features\": buffer}, indent=2, default=json_serial) + \"\\n\")\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Declaring some variables and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_roadtype = {\n",
    "    \"G\": 'Gemeente',\n",
    "    \"R\": 'Rijk',\n",
    "    \"P\": 'Provincie',\n",
    "    \"W\": 'Waterschap',\n",
    "    'T': 'Andere wegbeheerder',\n",
    "    '' : 'leeg'\n",
    "}\n",
    "\n",
    "dict_roadtype_to_color = {\n",
    "    \"G\": 'red',\n",
    "    \"R\": 'blue',\n",
    "    \"P\": 'green',\n",
    "    \"W\": 'magenta',\n",
    "    'T': 'yellow',\n",
    "    '' : 'leeg'\n",
    "}\n",
    "\n",
    "FEATURES_KEY = 'features'\n",
    "PROPERTIES_KEY = 'properties'\n",
    "GEOMETRY_KEY = 'geometry'\n",
    "COORDINATES_KEY = 'coordinates'\n",
    "WEGSOORT_KEY = 'WEGBEHSRT'\n",
    "\n",
    "MINIMUM_NO_POINTS_PER_TILE = 4\n",
    "POINTS_PER_METER = 10 # Changed by David from 0.1\n",
    "\n",
    "INPUT_FOLDER_TILES = '../datasets/image_tiles_200/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_to_dict(d1, d2, coordinates, rtype):\n",
    "    coordinate_ll_x = int((coordinates[0] // dx)*dx)\n",
    "    coordinate_ll_y = int((coordinates[1] // dy)*dy)\n",
    "    coordinate_ur_x = int((coordinates[0] // dx)*dx + dx)\n",
    "    coordinate_ur_y = int((coordinates[1] // dy)*dy + dy)\n",
    "    tile = \"{}_{}_{}_{}.jpg\".format(coordinate_ll_x, coordinate_ll_y, coordinate_ur_x, coordinate_ur_y)\n",
    "    \n",
    "    rel_coord_x = (coordinates[0] - coordinate_ll_x) / dx\n",
    "    rel_coord_y = (coordinates[1] - coordinate_ll_y) / dy\n",
    "    value = (rtype, rel_coord_x, rel_coord_y)\n",
    "    d1[tile].append(value)\n",
    "    d2[rtype].add(tile)\n",
    "\n",
    "def coord_is_in_bb(coord, bb):\n",
    "    x_min = bb[0]\n",
    "    y_min = bb[1]\n",
    "    x_max = bb[2]\n",
    "    y_max = bb[3]\n",
    "    return coord[0] > x_min and coord[0] < x_max and coord[1] > y_min and coord[1] < y_max\n",
    "\n",
    "def retrieve_roadtype(elem):\n",
    "    return elem[PROPERTIES_KEY][WEGSOORT_KEY]\n",
    "   \n",
    "def retrieve_coordinates(elem):\n",
    "    return elem[GEOMETRY_KEY][COORDINATES_KEY]\n",
    "\n",
    "def eucledian_distance(p1, p2):\n",
    "    diff = np.array(p2)-np.array(p1)\n",
    "    return np.linalg.norm(diff)\n",
    "\n",
    "def calculate_intermediate_points(p1, p2, no_points):\n",
    "    dx = (p2[0] - p1[0]) / (no_points + 1)\n",
    "    dy = (p2[1] - p1[1]) / (no_points + 1)\n",
    "    return [[p1[0] + i * dx, p1[1] +  i * dy] for i in range(1, no_points+1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Map contents of shapefile to the tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_wegvakken = json_filename\n",
    "dict_wegvakken = json.load(open(filename_wegvakken))[FEATURES_KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_tile_contents = defaultdict(list)\n",
    "d_roadtype_tiles = defaultdict(set)\n",
    "    \n",
    "for elem in dict_wegvakken:\n",
    "    coordinates = retrieve_coordinates(elem)\n",
    "    rtype = retrieve_roadtype(elem)\n",
    "    coordinates_in_bb = [coord for coord in coordinates if coord_is_in_bb(coord, bounding_box)]\n",
    "    if len(coordinates_in_bb)==1:\n",
    "        coord = coordinates_in_bb[0]\n",
    "        add_to_dict(d_tile_contents, d_roadtype_tiles, coord, rtype)\n",
    "    if len(coordinates_in_bb)>1:\n",
    "        add_to_dict(d_tile_contents, d_roadtype_tiles, coordinates_in_bb[0], rtype)\n",
    "        for ii in range(1,len(coordinates_in_bb)):\n",
    "            previous_coord = coordinates_in_bb[ii-1]\n",
    "            coord = coordinates_in_bb[ii]\n",
    "            add_to_dict(d_tile_contents, d_roadtype_tiles, coord, rtype)\n",
    "            \n",
    "            dist = eucledian_distance(previous_coord, coord)\n",
    "            no_intermediate_points = int(dist*POINTS_PER_METER)           \n",
    "            intermediate_coordinates = calculate_intermediate_points(previous_coord, coord, no_intermediate_points)\n",
    "            for intermediate_coord in intermediate_coordinates:\n",
    "                add_to_dict(d_tile_contents, d_roadtype_tiles, intermediate_coord, rtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Added by David:\n",
    "# 4a. Create dictionary of road-pixel matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize dictionary\n",
    "d_road_pixels = {}\n",
    "\n",
    "# Loop through all image tiles\n",
    "for ii in range(0,no_tiles_x):\n",
    "    for jj in range(0,no_tiles_y):\n",
    "        ll_x = x_min + ii*dx\n",
    "        ll_y = y_min + jj*dy\n",
    "        ur_x = ll_x + dx\n",
    "        ur_y = ll_y + dy\n",
    "        tile = \"{}_{}_{}_{}.jpg\".format(ll_x, ll_y, ur_x, ur_y)\n",
    "        filename = INPUT_FOLDER_TILES + tile   \n",
    "        # Extract list of road coordinates in tile\n",
    "        tile_contents = d_tile_contents[tile]\n",
    "\n",
    "        # Find pixel elements corresponding to roads\n",
    "        # Fill in matrix of values\n",
    "        pixel_mat = np.zeros((256,256))\n",
    "        for elem in tile_contents:\n",
    "            x = int(elem[1]*255)\n",
    "            y = int((1-elem[2])*255)\n",
    "            pixel_mat[y,x] = 1\n",
    "        d_road_pixels[tile] = pixel_mat\n",
    "        \n",
    "        #plt.spy(pixel_mat)\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b. Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0 = 95000\n",
    "y0 = 430000\n",
    "\n",
    "fig, axarr = plt.subplots(nrows=11,ncols=11, figsize=(16,16))\n",
    "\n",
    "for ii in range(0,11):\n",
    "    for jj in range(0,11):\n",
    "        ll_x = x0 + ii*dx\n",
    "        ll_y = y0 + jj*dy\n",
    "        ur_x = ll_x + dx\n",
    "        ur_y = ll_y + dy\n",
    "        tile = \"{}_{}_{}_{}.jpg\".format(ll_x, ll_y, ur_x, ur_y)\n",
    "        filename = INPUT_FOLDER_TILES + tile        \n",
    "        tile_contents = d_tile_contents[tile]\n",
    "        \n",
    "        ax = axarr[10-jj, ii]\n",
    "        image = ndimage.imread(filename)\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(rgb_image)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        for elem in tile_contents:\n",
    "            color = dict_roadtype_to_color[elem[0]]\n",
    "            x = elem[1]*256\n",
    "            y = (1-elem[2])*256\n",
    "            ax.scatter(x,y,c=color,s=10)\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DK checks\n",
    "image = \"95000_430000_95200_430200.jpg\"\n",
    "filename = INPUT_FOLDER_TILES + image\n",
    "image_view = ndimage.imread(filename).astype(np.float32)\n",
    "plt.imshow(image_view)\n",
    "plt.show()\n",
    "print(image_view.shape)\n",
    "print(type(image_view))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0 = 95400\n",
    "y0 = 432000\n",
    "POINTS_PER_METER = 0\n",
    "\n",
    "fig, axarr = plt.subplots(nrows=11,ncols=11, figsize=(16,16))\n",
    "\n",
    "for ii in range(0,11):\n",
    "    for jj in range(0,11):\n",
    "        ll_x = x0 + ii*dx\n",
    "        ll_y = y0 + jj*dy\n",
    "        ur_x = ll_x + dx\n",
    "        ur_y = ll_y + dy\n",
    "        tile = \"{}_{}_{}_{}.jpg\".format(ll_x, ll_y, ur_x, ur_y)\n",
    "        filename = INPUT_FOLDER_TILES + tile\n",
    "        tile_contents = d_tile_contents[tile]\n",
    "        \n",
    "        ax = axarr[10-jj, ii]\n",
    "        image = ndimage.imread(filename)\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(rgb_image)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        for elem in tile_contents:\n",
    "            color = dict_roadtype_to_color[elem[0]]\n",
    "            x = elem[1]*256\n",
    "            y = (1-elem[2])*256\n",
    "            ax.scatter(x,y,c=color,s=10)\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4c. Some statistics about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of roadtype G (Gemeente) there are 4963 tiles.\n",
      "Of roadtype W (Waterschap) there are 915 tiles.\n",
      "Of roadtype P (Provincie) there are 48 tiles.\n",
      "Of roadtype T (Andere wegbeheerder) there are 1 tiles.\n"
     ]
    }
   ],
   "source": [
    "for rtype in d_roadtype_tiles.keys():\n",
    "    roadtype = dict_roadtype[rtype]\n",
    "    no_tiles = len(d_roadtype_tiles[rtype])\n",
    "    print(\"Of roadtype {} ({}) there are {} tiles.\".format(rtype, roadtype, no_tiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prepare dataset for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels1, labels2):\n",
    "    permutation = np.random.permutation(dataset.shape[0])\n",
    "    print(permutation.shape)\n",
    "    print(dataset.shape)\n",
    "    print(labels1.shape)\n",
    "    print(labels2.shape)\n",
    "    randomized_dataset = dataset[permutation, :, :, :]\n",
    "    randomized_labels1 = labels1[permutation, :, :]\n",
    "    randomized_labels2 = labels2[permutation]\n",
    "    return randomized_dataset, randomized_labels1, randomized_labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images have been loaded.\n",
      "1000 images have been loaded.\n",
      "2000 images have been loaded.\n",
      "3000 images have been loaded.\n",
      "4000 images have been loaded.\n",
      "5000 images have been loaded.\n",
      "6000 images have been loaded.\n",
      "7000 images have been loaded.\n",
      "8000 images have been loaded.\n",
      "9000 images have been loaded.\n"
     ]
    }
   ],
   "source": [
    "image_width = 256\n",
    "image_height = 256\n",
    "image_depth = 3\n",
    "total_no_images = 10000\n",
    "\n",
    "image_files = os.listdir(INPUT_FOLDER_TILES)\n",
    "\n",
    "dataset = np.ndarray(shape=(total_no_images, image_width, image_height, image_depth), dtype=np.float32)\n",
    "labels_roadtype = []\n",
    "labels_roadpresence = np.ndarray(total_no_images, dtype=np.float32)\n",
    "labels_roadpixel = np.zeros((total_no_images, image_width, image_height))\n",
    "labels_filename = []\n",
    "\n",
    "for counter, image in enumerate(image_files):\n",
    "    filename = INPUT_FOLDER_TILES + image\n",
    "    labels_filename.append(image)\n",
    "    if image in list(d_tile_contents.keys()):\n",
    "        tile_contents = d_tile_contents[image]\n",
    "        roadtypes = sorted(list(set([elem[0] for elem in tile_contents])))\n",
    "        roadtype = \"_\".join(roadtypes)\n",
    "        labels_roadpresence[counter] = 1\n",
    "        labels_roadpixel[counter] = d_road_pixels[image]\n",
    "    else:\n",
    "        roadtype = ''\n",
    "        labels_roadpresence[counter] = 0\n",
    "        labels_roadpixel[counter,:,:] = np.zeros((image_width, image_height))\n",
    "    labels_roadtype.append(roadtype)\n",
    "\n",
    "    image_data = ndimage.imread(filename).astype(np.float32)\n",
    "    dataset[counter, :, :] = image_data\n",
    "    if counter % 1000 == 0:\n",
    "        print(\"{} images have been loaded.\".format(counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomizing dataset...\n",
      "(10000,)\n",
      "(10000, 256, 256, 3)\n",
      "(10000, 256, 256)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "labels_filename = np.array(labels_filename)\n",
    "print(\"Randomizing dataset...\")\n",
    "dataset, labels_roadpixels, labels_filename = randomize(dataset, labels_roadpixel, labels_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ec9e64965b01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m'test_labels_filename'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabels_filename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_test_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtotal_no_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m }\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "# Only used a subset to train and test\n",
    "start_train_dataset = 0\n",
    "start_valid_dataset = 8000\n",
    "start_test_dataset = 9000\n",
    "total_no_images = 10000\n",
    "\n",
    "output_pickle_file = '../data/sattelite_dataset_pixel.pickle'\n",
    "\n",
    "f = open(output_pickle_file, 'wb')\n",
    "save = {\n",
    "'train_dataset': dataset[start_train_dataset:start_valid_dataset,:,:,:],\n",
    "'train_labels_roadpixels': labels_roadpixels[start_train_dataset:start_valid_dataset,:,:],\n",
    "'train_labels_filename': labels_filename[start_train_dataset:start_valid_dataset],\n",
    "'valid_dataset': dataset[start_valid_dataset:start_test_dataset,:,:,:],\n",
    "'valid_labels_roadpixels': labels_roadpixels[start_valid_dataset:start_test_dataset,:,:],\n",
    "'valid_labels_filename': labels_filename[start_valid_dataset:start_test_dataset],\n",
    "'test_dataset': dataset[start_test_dataset:total_no_images,:,:,:],\n",
    "'test_labels_roadpixels': labels_roadpixels[start_test_dataset:total_no_images,:,:],\n",
    "'test_labels_filename': labels_filename[start_test_dataset:total_no_images]\n",
    "}\n",
    "pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()\n",
    "\n",
    "print(\"\\nsaved dataset to {}\".format(output_pickle_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to output\n",
    "# IGNORE if step above works\n",
    "def save_as_pickled_object(obj, filepath):\n",
    "    \"\"\"\n",
    "    This is a defensive way to write pickle.write, allowing for very large files on all platforms\n",
    "    \"\"\"\n",
    "    max_bytes = 2**31 - 1\n",
    "    bytes_out = pickle.dumps(obj, pickle.HIGHEST_PROTOCOL)\n",
    "    n_bytes = sys.getsizeof(bytes_out)\n",
    "    with open(filepath, 'wb') as f_out:\n",
    "        for idx in range(0, n_bytes, max_bytes):\n",
    "            f_out.write(bytes_out[idx:idx+max_bytes])\n",
    "            \n",
    "            \n",
    "output_pickle_file = os.path.expanduser('~/Downloads/sattelite_dataset_pixel.pickle')\n",
    "save_as_pickled_object(save, output_pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. The Convolutional neural network part (Using alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from cnn_models.alexnet import * \n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = './data/sattelite_dataset.pickle'\n",
    "f = open(pickle_file, 'rb')\n",
    "save = pickle.load(f)\n",
    "\n",
    "train_dataset = save['train_dataset'].astype(dtype = np.float32)\n",
    "train_labels = save['train_labels_roadpresence'].astype(dtype = np.float32)\n",
    "valid_dataset = save['valid_dataset'].astype(dtype = np.float32)\n",
    "valid_labels = save['valid_labels_roadpresence'].astype(dtype = np.float32)\n",
    "test_dataset = save['test_dataset'].astype(dtype = np.float32)\n",
    "test_labels = save['test_labels_roadpresence'].astype(dtype = np.float32)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_labels = len(np.unique(train_labels))\n",
    "print(np.unique(train_labels))\n",
    "image_width = 256\n",
    "image_height = 256\n",
    "image_depth = 3\n",
    "#num_steps = 501\n",
    "#display_step = 10\n",
    "learning_rate = 0.01\n",
    "batch_size = 5\n",
    "#lambda_loss_amount = 0.0015\n",
    "train_accuracies, test_accuracies, valid_accuracies = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = alexnet(input_shape = (256, 256, 3), labels = 2)\n",
    "#adam = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "sgd = optimizers.SGD(lr=learning_rate, clipvalue=0.5)\n",
    "model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "train_history = model.fit(train_dataset, train_labels, validation_data = (valid_dataset, valid_labels), epochs = 20, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_accuracies = train_history.history['acc']\n",
    "valid_accuracies = train_history.history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_accuracies)\n",
    "print(valid_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking gradient values of the loss wrt weights of last but one layer\n",
    "weights = model.trainable_weights # weight tensors\n",
    "grads = K.function([model.layers[0].input],K.gradients(model.layers[-1].output, weights[-2]))\n",
    "gradients = grads([train_dataset])[0]\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Output values before activation in last layer\n",
    "get_last_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[-2].output])\n",
    "layer_output = get_last_layer_output([train_dataset])[0]\n",
    "print(layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_results = model.evaluate(test_dataset, test_labels)\n",
    "print (\"Loss = \" + str(test_results[0]))\n",
    "print (\"Test Accuracy = \" + str(test_results[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

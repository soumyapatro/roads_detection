{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full explanation of the code, visit http://ataspinar.com/2017/12/04/using-convolutional-neural-networks-to-detect-features-in-sattelite-images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import requests\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy import ndimage\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#We are using owslib to download images from a WMS Service\n",
    "#install with 'pip install owslib'\n",
    "\n",
    "import owslib\n",
    "\n",
    "from owslib.wms import WebMapService\n",
    "\n",
    "#pyshp is necessary for loading and saving shapefiles\n",
    "#install with 'pip install pyshp'\n",
    "import shapefile\n",
    "\n",
    "# Install opencv with 'pip install opencv-python'\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min = 90000\n",
    "y_min = 427000\n",
    "dx, dy = 200, 200\n",
    "no_tiles_x = 100\n",
    "no_tiles_y = 100\n",
    "total_no_tiles = no_tiles_x * no_tiles_y\n",
    "\n",
    "x_max = x_min + no_tiles_x * dx\n",
    "y_max = y_min + no_tiles_y * dy\n",
    "bounding_box = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "TILE_FOLDER = \"./datasets/image_tiles_200/\"\n",
    "URL_TILES = \"https://geodata.nationaalgeoregister.nl/luchtfoto/rgb/wms?request=GetCapabilities\"\n",
    "\n",
    "URL_SHP = 'https://www.rijkswaterstaat.nl/apps/geoservices/geodata/dmc/nwb-wegen/geogegevens/shapefile/Nederland_totaal/01-08-2017/Wegvakken/Wegvakken.shp'\n",
    "URL_PRF = 'https://www.rijkswaterstaat.nl/apps/geoservices/geodata/dmc/nwb-wegen/geogegevens/shapefile/Nederland_totaal/01-08-2017/Wegvakken/Wegvakken.prj'\n",
    "URL_DBF = 'https://www.rijkswaterstaat.nl/apps/geoservices/geodata/dmc/nwb-wegen/geogegevens/shapefile/Nederland_totaal/01-08-2017/Wegvakken/Wegvakken.dbf'\n",
    "URL_SHX = 'https://www.rijkswaterstaat.nl/apps/geoservices/geodata/dmc/nwb-wegen/geogegevens/shapefile/Nederland_totaal/01-08-2017/Wegvakken/Wegvakken.shx'\n",
    "\n",
    "URLS_SHAPEFILES = [URL_SHP, URL_PRF, URL_DBF, URL_SHX]\n",
    "\n",
    "DATA_FOLDER = \"./data/nwb_wegvakken/\"\n",
    "\n",
    "json_filename = DATA_FOLDER + '2017_09_wegvakken.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Downloading the image tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip if you already have the image tiles. Will take ~ 2hours.\n",
    "wms = WebMapService(URL_TILES, version='1.1.1')\n",
    "\n",
    "if not os.path.exists(TILE_FOLDER):\n",
    "    os.makedirs(TILE_FOLDER)\n",
    "\n",
    "for ii in range(25,no_tiles_x):\n",
    "    print(ii)\n",
    "    for jj in range(0,no_tiles_y):\n",
    "        ll_x_ = x_min + ii*dx\n",
    "        ll_y_ = y_min + jj*dy\n",
    "        bbox = (ll_x_, ll_y_, ll_x_ + dx, ll_y_ + dy) \n",
    "        img = wms.getmap(layers=['Actueel_ortho25'], srs='EPSG:28992', bbox=bbox, size=(256, 256), format='image/jpeg', transparent=True)\n",
    "        filename = \"{}{}_{}_{}_{}.jpg\".format(TILE_FOLDER, bbox[0], bbox[1], bbox[2], bbox[3])\n",
    "        out = open(filename, 'wb')\n",
    "        out.write(img.read())\n",
    "        out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Downloading the shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip if you already have the shapefiles. Will take ~ 1hour.\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.makedirs(DATA_FOLDER)\n",
    "\n",
    "for url in URLS_SHAPEFILES:\n",
    "    filename = url.split('/')[-1]\n",
    "    print(\"Downloading file {}\".format(filename))\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(DATA_FOLDER + filename, 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading shapefile and converting to (GEO)Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def json_serial(obj):\n",
    "    \"\"\"JSON serializer for objects not serializable by default json code\"\"\"\n",
    "\n",
    "    if isinstance(obj, (datetime, date)):\n",
    "        serial = obj.isoformat()\n",
    "        return serial\n",
    "    if isinstance(obj, bytes):\n",
    "        return {'__class__': 'bytes',\n",
    "                '__value__': list(obj)}\n",
    "    raise TypeError (\"Type %s not serializable\" % type(obj))\n",
    "\n",
    "reader = shapefile.Reader(DATA_FOLDER + 'Wegvakken.shp')\n",
    "fields = reader.fields[1:]\n",
    "field_names = [field[0] for field in fields]\n",
    "\n",
    "buffer = []\n",
    "for sr in reader.shapeRecords()[:500000]:\n",
    "    atr = dict(zip(field_names, sr.record))\n",
    "    geom = sr.shape.__geo_interface__\n",
    "    buffer.append(dict(type=\"Feature\", geometry=geom, properties=atr)) \n",
    "\n",
    "\n",
    "json_file = open(json_filename, \"w\")\n",
    "json_file.write(json.dumps({\"type\": \"FeatureCollection\", \"features\": buffer}, indent=2, default=json_serial) + \"\\n\")\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Declaring some variables and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_roadtype = {\n",
    "    \"G\": 'Gemeente',\n",
    "    \"R\": 'Rijk',\n",
    "    \"P\": 'Provincie',\n",
    "    \"W\": 'Waterschap',\n",
    "    'T': 'Andere wegbeheerder',\n",
    "    '' : 'leeg'\n",
    "}\n",
    "\n",
    "dict_roadtype_to_color = {\n",
    "    \"G\": 'red',\n",
    "    \"R\": 'blue',\n",
    "    \"P\": 'green',\n",
    "    \"W\": 'magenta',\n",
    "    'T': 'yellow',\n",
    "    '' : 'leeg'\n",
    "}\n",
    "\n",
    "FEATURES_KEY = 'features'\n",
    "PROPERTIES_KEY = 'properties'\n",
    "GEOMETRY_KEY = 'geometry'\n",
    "COORDINATES_KEY = 'coordinates'\n",
    "WEGSOORT_KEY = 'WEGBEHSRT'\n",
    "\n",
    "MINIMUM_NO_POINTS_PER_TILE = 4\n",
    "POINTS_PER_METER = 0.1\n",
    "\n",
    "INPUT_FOLDER_TILES = './datasets/image_tiles_200/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dict(d1, d2, coordinates, rtype):\n",
    "    coordinate_ll_x = int((coordinates[0] // dx)*dx)\n",
    "    coordinate_ll_y = int((coordinates[1] // dy)*dy)\n",
    "    coordinate_ur_x = int((coordinates[0] // dx)*dx + dx)\n",
    "    coordinate_ur_y = int((coordinates[1] // dy)*dy + dy)\n",
    "    tile = \"{}_{}_{}_{}.jpg\".format(coordinate_ll_x, coordinate_ll_y, coordinate_ur_x, coordinate_ur_y)\n",
    "    \n",
    "    rel_coord_x = (coordinates[0] - coordinate_ll_x) / dx\n",
    "    rel_coord_y = (coordinates[1] - coordinate_ll_y) / dy\n",
    "    value = (rtype, rel_coord_x, rel_coord_y)\n",
    "    d1[tile].append(value)\n",
    "    d2[rtype].add(tile)\n",
    "\n",
    "def coord_is_in_bb(coord, bb):\n",
    "    x_min = bb[0]\n",
    "    y_min = bb[1]\n",
    "    x_max = bb[2]\n",
    "    y_max = bb[3]\n",
    "    return coord[0] > x_min and coord[0] < x_max and coord[1] > y_min and coord[1] < y_max\n",
    "\n",
    "def retrieve_roadtype(elem):\n",
    "    return elem[PROPERTIES_KEY][WEGSOORT_KEY]\n",
    "   \n",
    "def retrieve_coordinates(elem):\n",
    "    return elem[GEOMETRY_KEY][COORDINATES_KEY]\n",
    "\n",
    "def eucledian_distance(p1, p2):\n",
    "    diff = np.array(p2)-np.array(p1)\n",
    "    return np.linalg.norm(diff)\n",
    "\n",
    "def calculate_intermediate_points(p1, p2, no_points):\n",
    "    dx = (p2[0] - p1[0]) / (no_points + 1)\n",
    "    dy = (p2[1] - p1[1]) / (no_points + 1)\n",
    "    return [[p1[0] + i * dx, p1[1] +  i * dy] for i in range(1, no_points+1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Map contents of shapefile to the tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_wegvakken = json_filename\n",
    "dict_wegvakken = json.load(open(filename_wegvakken))[FEATURES_KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tile_contents = defaultdict(list)\n",
    "d_roadtype_tiles = defaultdict(set)\n",
    "    \n",
    "for elem in dict_wegvakken:\n",
    "    coordinates = retrieve_coordinates(elem)\n",
    "    rtype = retrieve_roadtype(elem)\n",
    "    coordinates_in_bb = [coord for coord in coordinates if coord_is_in_bb(coord, bounding_box)]\n",
    "    if len(coordinates_in_bb)==1:\n",
    "        coord = coordinates_in_bb[0]\n",
    "        add_to_dict(d_tile_contents, d_roadtype_tiles, coord, rtype)\n",
    "    if len(coordinates_in_bb)>1:\n",
    "        add_to_dict(d_tile_contents, d_roadtype_tiles, coordinates_in_bb[0], rtype)\n",
    "        for ii in range(1,len(coordinates_in_bb)):\n",
    "            previous_coord = coordinates_in_bb[ii-1]\n",
    "            coord = coordinates_in_bb[ii]\n",
    "            add_to_dict(d_tile_contents, d_roadtype_tiles, coord, rtype)\n",
    "            \n",
    "            dist = eucledian_distance(previous_coord, coord)\n",
    "            no_intermediate_points = int(dist*POINTS_PER_METER)           \n",
    "            intermediate_coordinates = calculate_intermediate_points(previous_coord, coord, no_intermediate_points)\n",
    "            for intermediate_coord in intermediate_coordinates:\n",
    "                add_to_dict(d_tile_contents, d_roadtype_tiles, intermediate_coord, rtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b. Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 95000\n",
    "y0 = 430000\n",
    "\n",
    "fig, axarr = plt.subplots(nrows=11,ncols=11, figsize=(16,16))\n",
    "\n",
    "for ii in range(0,11):\n",
    "    for jj in range(0,11):\n",
    "        ll_x = x0 + ii*dx\n",
    "        ll_y = y0 + jj*dy\n",
    "        ur_x = ll_x + dx\n",
    "        ur_y = ll_y + dy\n",
    "        tile = \"{}_{}_{}_{}.jpg\".format(ll_x, ll_y, ur_x, ur_y)\n",
    "        filename = INPUT_FOLDER_TILES + tile        \n",
    "        tile_contents = d_tile_contents[tile]\n",
    "        \n",
    "        ax = axarr[10-jj, ii]\n",
    "        image = ndimage.imread(filename)\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(rgb_image)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        for elem in tile_contents:\n",
    "            color = dict_roadtype_to_color[elem[0]]\n",
    "            x = elem[1]*256\n",
    "            y = (1-elem[2])*256\n",
    "            ax.scatter(x,y,c=color,s=10)\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 95400\n",
    "y0 = 432000\n",
    "POINTS_PER_METER = 0\n",
    "\n",
    "fig, axarr = plt.subplots(nrows=11,ncols=11, figsize=(16,16))\n",
    "\n",
    "for ii in range(0,11):\n",
    "    for jj in range(0,11):\n",
    "        ll_x = x0 + ii*dx\n",
    "        ll_y = y0 + jj*dy\n",
    "        ur_x = ll_x + dx\n",
    "        ur_y = ll_y + dy\n",
    "        tile = \"{}_{}_{}_{}.jpg\".format(ll_x, ll_y, ur_x, ur_y)\n",
    "        filename = INPUT_FOLDER_TILES + tile\n",
    "        tile_contents = d_tile_contents[tile]\n",
    "        \n",
    "        ax = axarr[10-jj, ii]\n",
    "        image = ndimage.imread(filename)\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(rgb_image)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        for elem in tile_contents:\n",
    "            color = dict_roadtype_to_color[elem[0]]\n",
    "            x = elem[1]*256\n",
    "            y = (1-elem[2])*256\n",
    "            ax.scatter(x,y,c=color,s=10)\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4c. Some statistics about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} tiles containing roads.\".format(len(d_tile_contents.keys())))\n",
    "\n",
    "for rtype in d_roadtype_tiles.keys():\n",
    "    roadtype = dict_roadtype[rtype]\n",
    "    no_tiles = len(d_roadtype_tiles[rtype])\n",
    "    print(\"Of roadtype {} ({}) there are {} tiles.\".format(rtype, roadtype, no_tiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prepare dataset for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "def onehot_encode_labels(labels):\n",
    "    list_possible_labels = list(np.unique(labels))\n",
    "    encoded_labels = map(lambda x: list_possible_labels.index(x), labels)\n",
    "    return encoded_labels\n",
    "\n",
    "def randomize(dataset, labels1, labels2, labels3):\n",
    "    permutation = np.random.permutation(dataset.shape[0])\n",
    "    print(permutation.shape)\n",
    "    print(dataset.shape)\n",
    "    print(labels1.shape)\n",
    "    print(labels2.shape)\n",
    "    print(labels3.shape)\n",
    "    randomized_dataset = dataset[permutation, :, :, :]\n",
    "    randomized_labels1 = labels1[permutation]\n",
    "    randomized_labels2 = labels2[permutation]\n",
    "    randomized_labels3 = labels3[permutation]\n",
    "    return randomized_dataset, randomized_labels1, randomized_labels2, randomized_labels3\n",
    "\n",
    "def one_hot_encode(np_array, num_unique_labels):\n",
    "    return (np.arange(num_unique_labels) == np_array[:,None]).astype(np.float32)\n",
    "\n",
    "def reformat_data(dataset, labels1, labels2, labels3):\n",
    "    dataset, labels1, labels2, labels3 = randomize(dataset, labels1, labels2, labels3)\n",
    "    num_unique_labels1 = len(np.unique(labels1))\n",
    "    num_unique_labels2 = len(np.unique(labels2))\n",
    "    labels1 = one_hot_encode(labels1, num_unique_labels1)\n",
    "    labels2 = one_hot_encode(labels2, num_unique_labels2)\n",
    "    return dataset, labels1, labels2, labels3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 256\n",
    "image_height = 256\n",
    "image_depth = 3\n",
    "total_no_images = 7500\n",
    "\n",
    "image_files = os.listdir(INPUT_FOLDER_TILES)\n",
    "\n",
    "dataset = np.ndarray(shape=(total_no_images, image_width, image_height, image_depth), dtype=np.float32)\n",
    "labels_roadtype = []\n",
    "labels_roadpresence = np.ndarray(total_no_images, dtype=np.float32)\n",
    "labels_filename = []\n",
    "\n",
    "for counter, image in enumerate(image_files):\n",
    "    filename = INPUT_FOLDER_TILES + image\n",
    "    labels_filename.append(image)\n",
    "    if image in list(d_tile_contents.keys()):\n",
    "        tile_contents = d_tile_contents[image]\n",
    "        roadtypes = sorted(list(set([elem[0] for elem in tile_contents])))\n",
    "        roadtype = \"_\".join(roadtypes)\n",
    "        labels_roadpresence[counter] = 1\n",
    "    else:\n",
    "        roadtype = ''\n",
    "        labels_roadpresence[counter] = 0\n",
    "    labels_roadtype.append(roadtype)\n",
    "\n",
    "    image_data = ndimage.imread(filename).astype(np.float32)\n",
    "    dataset[counter, :, :] = image_data\n",
    "    if counter % 1000 == 0:\n",
    "        print(\"{} images have been loaded.\".format(counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_filename = np.array(labels_filename)\n",
    "labels_roadtype_ohe = np.array(list(onehot_encode_labels(labels_roadtype)))\n",
    "print(\"Randomizing dataset...\")\n",
    "dataset, labels_roadpresence, labels_roadtype_ohe, labels_filename = reformat_data(dataset, labels_roadpresence, labels_roadtype_ohe, labels_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only used a subset to train and test\n",
    "start_train_dataset = 0\n",
    "start_valid_dataset = 12\n",
    "start_test_dataset = 16\n",
    "total_no_images = 20\n",
    "\n",
    "output_pickle_file = './data/sattelite_dataset.pickle'\n",
    "\n",
    "f = open(output_pickle_file, 'wb')\n",
    "save = {\n",
    "'train_dataset': dataset[start_train_dataset:start_valid_dataset,:,:,:],\n",
    "'train_labels_roadtype': labels_roadtype[start_train_dataset:start_valid_dataset],\n",
    "'train_labels_roadpresence': labels_roadpresence[start_train_dataset:start_valid_dataset],\n",
    "'train_labels_filename': labels_filename[start_train_dataset:start_valid_dataset],\n",
    "'valid_dataset': dataset[start_valid_dataset:start_test_dataset,:,:,:],\n",
    "'valid_labels_roadtype': labels_roadtype[start_valid_dataset:start_test_dataset],\n",
    "'valid_labels_roadpresence': labels_roadpresence[start_valid_dataset:start_test_dataset],\n",
    "'valid_labels_filename': labels_filename[start_valid_dataset:start_test_dataset],\n",
    "'test_dataset': dataset[start_test_dataset:total_no_images,:,:,:],\n",
    "'test_labels_roadtype': labels_roadtype[start_test_dataset:total_no_images],\n",
    "'test_labels_roadpresence': labels_roadpresence[start_test_dataset:total_no_images],\n",
    "'test_labels_filename': labels_filename[start_test_dataset:total_no_images]\n",
    "}\n",
    "pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()\n",
    "\n",
    "print(\"\\nsaved dataset to {}\".format(output_pickle_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. The Convolutional neural network part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from cnn_models.alexnet import * \n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = './data/sattelite_dataset.pickle'\n",
    "f = open(pickle_file, 'rb')\n",
    "save = pickle.load(f)\n",
    "\n",
    "train_dataset = save['train_dataset'].astype(dtype = np.float32)\n",
    "train_labels = save['train_labels_roadpresence'].astype(dtype = np.float32)\n",
    "valid_dataset = save['valid_dataset'].astype(dtype = np.float32)\n",
    "valid_labels = save['valid_labels_roadpresence'].astype(dtype = np.float32)\n",
    "test_dataset = save['test_dataset'].astype(dtype = np.float32)\n",
    "test_labels = save['test_labels_roadpresence'].astype(dtype = np.float32)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.]\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(np.unique(train_labels))\n",
    "print(np.unique(train_labels))\n",
    "image_width = 256\n",
    "image_height = 256\n",
    "image_depth = 3\n",
    "num_steps = 501\n",
    "display_step = 10\n",
    "learning_rate = 0.01\n",
    "batch_size = 16\n",
    "lambda_loss_amount = 0.0015\n",
    "train_accuracies, test_accuracies, valid_accuracies = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 40 samples\n",
      "Epoch 1/20\n",
      "120/120 [==============================] - 40s 330ms/step - loss: 6.4646 - acc: 0.5667 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 2/20\n",
      "120/120 [==============================] - 38s 317ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 3/20\n",
      "120/120 [==============================] - 40s 330ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 4/20\n",
      "120/120 [==============================] - 40s 330ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 5/20\n",
      "120/120 [==============================] - 40s 334ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 6/20\n",
      "120/120 [==============================] - 43s 357ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 7/20\n",
      "120/120 [==============================] - 40s 337ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 8/20\n",
      "120/120 [==============================] - 41s 343ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 9/20\n",
      "120/120 [==============================] - 42s 352ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 10/20\n",
      "120/120 [==============================] - 40s 337ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 11/20\n",
      "120/120 [==============================] - 40s 335ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 12/20\n",
      "120/120 [==============================] - 40s 333ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 13/20\n",
      "120/120 [==============================] - 40s 337ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 14/20\n",
      "120/120 [==============================] - 39s 329ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 15/20\n",
      "120/120 [==============================] - 39s 322ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 16/20\n",
      "120/120 [==============================] - 38s 320ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 17/20\n",
      "120/120 [==============================] - 39s 326ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 18/20\n",
      "120/120 [==============================] - 40s 329ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 19/20\n",
      "120/120 [==============================] - 41s 343ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n",
      "Epoch 20/20\n",
      "120/120 [==============================] - 40s 333ms/step - loss: 6.5457 - acc: 0.5917 - val_loss: 8.4159 - val_acc: 0.4750\n"
     ]
    }
   ],
   "source": [
    "model = alexnet(input_shape = (256, 256, 3), labels = 2)\n",
    "adam = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "train_history = model.fit(train_dataset, train_labels, validation_data = (valid_dataset, valid_labels), epochs = 20, batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies = train_history.history['acc']\n",
    "valid_accuracies = train_history.history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]]\n",
      "[0.47500000707805157, 0.47500000707805157, 0.47500000707805157, 0.47500000707805157, 0.47500000707805157, 0.47500000707805157, 0.47500000707805157, 0.47500000707805157, 0.47500000707805157, 0.47500000707805157, 0.47500000707805157, 0.47500000707805157, 0.47500000707805157, 0.47500000707805157, 0.47500000707805157, 0.47500000707805157, 0.47500000707805157, 0.47500000707805157, 0.47500000707805157, 0.47500000707805157]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels)\n",
    "print(valid_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " ..., \n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "weights = model.trainable_weights # weight tensors\n",
    "grads = K.function([model.layers[0].input],K.gradients(model.layers[-1].output, weights[-2]))\n",
    "gradients = grads([train_dataset])[0]\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -292272.9375     311115.625   ]\n",
      " [ -815607.1875     889128.1875  ]\n",
      " [ -243142.28125    258319.546875]\n",
      " [ -395386.96875    421450.90625 ]\n",
      " [ -849302.1875     910296.875   ]\n",
      " [ -290485.34375    313633.09375 ]\n",
      " [ -332357.0625     358491.875   ]\n",
      " [ -514458.25       545902.375   ]\n",
      " [ -284530.15625    312091.0625  ]\n",
      " [-1007894.        1095403.5     ]\n",
      " [ -324934.65625    345392.0625  ]\n",
      " [ -471462.96875    504256.375   ]\n",
      " [ -332164.0625     361446.8125  ]\n",
      " [ -277957.21875    301819.65625 ]\n",
      " [ -615009.4375     664621.5     ]\n",
      " [ -270748.96875    293280.25    ]\n",
      " [ -264691.6875     284377.1875  ]\n",
      " [ -342803.375      374750.75    ]\n",
      " [-1899605.625     2060386.      ]\n",
      " [ -345179.78125    366325.0625  ]\n",
      " [ -246424.0625     259784.25    ]\n",
      " [ -418663.125      442421.125   ]\n",
      " [ -326750.5        347481.375   ]\n",
      " [-2900669.        3088057.5     ]\n",
      " [-1614885.        1724073.375   ]\n",
      " [-2987407.75      3179170.5     ]\n",
      " [ -760320.6875     809765.75    ]\n",
      " [ -275681.46875    294308.      ]\n",
      " [ -270408.46875    286040.5     ]\n",
      " [ -794900.5625     877411.8125  ]\n",
      " [ -484958.90625    529994.1875  ]\n",
      " [ -431986.625      473402.1875  ]\n",
      " [-1201720.375     1299505.625   ]\n",
      " [ -241053.09375    270226.53125 ]\n",
      " [ -304936.125      328273.5     ]\n",
      " [ -331300.15625    354940.875   ]\n",
      " [ -227121.640625   250391.53125 ]\n",
      " [ -273853.90625    294411.      ]\n",
      " [ -354614.875      383922.40625 ]\n",
      " [ -434457.75       468423.5625  ]\n",
      " [ -324800.5        344976.40625 ]\n",
      " [ -344762.3125     365576.6875  ]\n",
      " [ -335836.0625     359436.75    ]\n",
      " [ -288533.         311636.4375  ]\n",
      " [ -331683.         366727.75    ]\n",
      " [ -386701.125      408930.375   ]\n",
      " [ -298958.53125    314963.5     ]\n",
      " [ -349672.21875    369707.3125  ]\n",
      " [ -278173.125      298498.0625  ]\n",
      " [ -538158.1875     578921.9375  ]\n",
      " [ -251479.390625   265639.6875  ]\n",
      " [ -273054.25       282251.09375 ]\n",
      " [ -570604.4375     607189.6875  ]\n",
      " [ -385172.6875     424297.5625  ]\n",
      " [ -213659.671875   229498.921875]\n",
      " [ -618956.6875     661499.1875  ]\n",
      " [ -465335.4375     488706.46875 ]\n",
      " [ -422557.0625     461505.21875 ]\n",
      " [ -310650.5        327946.78125 ]\n",
      " [ -412939.15625    434137.84375 ]\n",
      " [ -468282.59375    511956.4375  ]\n",
      " [ -256955.1875     278629.78125 ]\n",
      " [ -698067.125      743899.5     ]\n",
      " [ -493780.21875    523332.3125  ]\n",
      " [ -306422.28125    329606.375   ]\n",
      " [ -256605.46875    269400.90625 ]\n",
      " [ -265324.84375    291234.21875 ]\n",
      " [ -374300.625      409519.8125  ]\n",
      " [ -230264.828125   250794.6875  ]\n",
      " [ -435286.875      470608.0625  ]\n",
      " [ -273779.3125     289345.71875 ]\n",
      " [ -262454.28125    288324.15625 ]\n",
      " [ -587586.4375     627172.0625  ]\n",
      " [ -487693.96875    513733.125   ]\n",
      " [ -252744.65625    281825.21875 ]\n",
      " [ -352540.34375    372606.3125  ]\n",
      " [ -944974.5       1036540.1875  ]\n",
      " [ -337893.1875     364710.90625 ]\n",
      " [ -361331.0625     389830.75    ]\n",
      " [ -328573.65625    341644.5     ]\n",
      " [ -264326.15625    286365.96875 ]\n",
      " [ -322328.3125     337726.4375  ]\n",
      " [ -309235.5        332042.375   ]\n",
      " [ -306029.46875    322750.53125 ]\n",
      " [ -709577.625      761360.0625  ]\n",
      " [ -269247.5625     289140.4375  ]\n",
      " [ -363847.90625    391029.375   ]\n",
      " [ -593871.5625     634518.875   ]\n",
      " [ -219462.03125    235438.984375]\n",
      " [ -322683.34375    349069.03125 ]\n",
      " [ -474831.9375     516632.78125 ]\n",
      " [ -670487.0625     714611.      ]\n",
      " [ -376370.9375     405167.125   ]\n",
      " [ -273358.1875     293197.15625 ]\n",
      " [ -367907.46875    394161.03125 ]\n",
      " [ -329964.3125     348179.875   ]\n",
      " [ -447072.6875     477079.125   ]\n",
      " [ -286583.15625    310979.1875  ]\n",
      " [ -292141.71875    317456.8125  ]\n",
      " [ -289767.6875     304754.625   ]\n",
      " [ -681627.8125     703954.1875  ]\n",
      " [-1341863.625     1396504.125   ]\n",
      " [ -297175.         313044.21875 ]\n",
      " [ -341951.6875     361617.65625 ]\n",
      " [ -302266.46875    320911.15625 ]\n",
      " [ -259107.671875   277954.09375 ]\n",
      " [ -281290.25       298759.03125 ]\n",
      " [ -333341.40625    361319.65625 ]\n",
      " [ -448668.25       470851.1875  ]\n",
      " [ -336757.875      356747.65625 ]\n",
      " [ -256316.265625   281360.3125  ]\n",
      " [ -565395.25       608323.375   ]\n",
      " [ -307376.5625     334231.3125  ]\n",
      " [ -262953.65625    277717.34375 ]\n",
      " [ -508887.625      539455.625   ]\n",
      " [ -341548.40625    365453.375   ]\n",
      " [ -323727.90625    347566.5625  ]\n",
      " [ -262152.125      275304.3125  ]\n",
      " [ -296609.625      319602.5625  ]\n",
      " [ -316424.         344229.78125 ]]\n"
     ]
    }
   ],
   "source": [
    "get_last_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[-2].output])\n",
    "layer_output = get_last_layer_output([train_dataset])[0]\n",
    "print(layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 3s 67ms/step\n",
      "Loss = 6.41209583282\n",
      "Test Accuracy = 0.6\n"
     ]
    }
   ],
   "source": [
    "test_results = model.evaluate(test_dataset, test_labels)\n",
    "print (\"Loss = \" + str(test_results[0]))\n",
    "print (\"Test Accuracy = \" + str(test_results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_points(points, stepsize = 10):\n",
    "    averaged_points = []\n",
    "    for ii in range(stepsize,len(points),stepsize):\n",
    "        subsection  = points[ii-stepsize:ii]\n",
    "        average = np.nanmean(subsection)\n",
    "        averaged_points.append(average)\n",
    "    return averaged_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 501\n",
    "ylimit = [0,100]\n",
    "labels = ['Train accuracy', 'Validation accuracy']\n",
    "ylabel = \"Accuracy [%]\"\n",
    "xlabel = \"Number of Iterations\"\n",
    "title = \"Accuracy of road detection in Aerial Images\"\n",
    "colors = ['r', 'b']\n",
    "\n",
    "list_accuracies = [train_accuracies, valid_accuracies]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.set_ylim(ylimit)\n",
    "ax.set_ylabel(ylabel, fontsize=16)\n",
    "ax.set_xlabel(xlabel, fontsize=16)\n",
    "ax.set_title(title, fontsize=20)\n",
    "\n",
    "\n",
    "for ii, accuracies in enumerate(list_accuracies):\n",
    "    color = colors[ii]\n",
    "    label = labels[ii]\n",
    "    if ii > 0:\n",
    "        y_values = accuracies\n",
    "        x_values = range(0,num_steps, 10)\n",
    "        ax.plot(x_values, y_values, '.-{}'.format(color), label = label)\n",
    "    else:\n",
    "        y_values_ = accuracies\n",
    "        y_values = average_points(y_values_, 5)\n",
    "        x_values = range(1,len(y_values_),5)\n",
    "        ax.plot(x_values, y_values, '.{}'.format(color), label = label)\n",
    "ax.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

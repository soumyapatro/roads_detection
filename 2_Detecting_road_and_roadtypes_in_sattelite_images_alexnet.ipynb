{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full explanation of the code, visit http://ataspinar.com/2017/12/04/using-convolutional-neural-networks-to-detect-features-in-sattelite-images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import requests\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy import ndimage\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#We are using owslib to download images from a WMS Service\n",
    "#install with 'pip install owslib'\n",
    "\n",
    "import owslib\n",
    "\n",
    "from owslib.wms import WebMapService\n",
    "\n",
    "#pyshp is necessary for loading and saving shapefiles\n",
    "#install with 'pip install pyshp'\n",
    "import shapefile\n",
    "\n",
    "# Install opencv with 'pip install opencv-python'\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min = 90000\n",
    "y_min = 427000\n",
    "dx, dy = 200, 200\n",
    "no_tiles_x = 100\n",
    "no_tiles_y = 100\n",
    "total_no_tiles = no_tiles_x * no_tiles_y\n",
    "\n",
    "x_max = x_min + no_tiles_x * dx\n",
    "y_max = y_min + no_tiles_y * dy\n",
    "bounding_box = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "TILE_FOLDER = \"./datasets/image_tiles_200/\"\n",
    "URL_TILES = \"https://geodata.nationaalgeoregister.nl/luchtfoto/rgb/wms?request=GetCapabilities\"\n",
    "\n",
    "URL_SHP = 'https://www.rijkswaterstaat.nl/apps/geoservices/geodata/dmc/nwb-wegen/geogegevens/shapefile/Nederland_totaal/01-08-2017/Wegvakken/Wegvakken.shp'\n",
    "URL_PRF = 'https://www.rijkswaterstaat.nl/apps/geoservices/geodata/dmc/nwb-wegen/geogegevens/shapefile/Nederland_totaal/01-08-2017/Wegvakken/Wegvakken.prj'\n",
    "URL_DBF = 'https://www.rijkswaterstaat.nl/apps/geoservices/geodata/dmc/nwb-wegen/geogegevens/shapefile/Nederland_totaal/01-08-2017/Wegvakken/Wegvakken.dbf'\n",
    "URL_SHX = 'https://www.rijkswaterstaat.nl/apps/geoservices/geodata/dmc/nwb-wegen/geogegevens/shapefile/Nederland_totaal/01-08-2017/Wegvakken/Wegvakken.shx'\n",
    "\n",
    "URLS_SHAPEFILES = [URL_SHP, URL_PRF, URL_DBF, URL_SHX]\n",
    "\n",
    "DATA_FOLDER = \"./data/nwb_wegvakken/\"\n",
    "\n",
    "json_filename = DATA_FOLDER + '2017_09_wegvakken.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Downloading the image tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip if you already have the image tiles. Will take ~ 2hours.\n",
    "wms = WebMapService(URL_TILES, version='1.1.1')\n",
    "\n",
    "if not os.path.exists(TILE_FOLDER):\n",
    "    os.makedirs(TILE_FOLDER)\n",
    "\n",
    "for ii in range(25,no_tiles_x):\n",
    "    print(ii)\n",
    "    for jj in range(0,no_tiles_y):\n",
    "        ll_x_ = x_min + ii*dx\n",
    "        ll_y_ = y_min + jj*dy\n",
    "        bbox = (ll_x_, ll_y_, ll_x_ + dx, ll_y_ + dy) \n",
    "        img = wms.getmap(layers=['Actueel_ortho25'], srs='EPSG:28992', bbox=bbox, size=(256, 256), format='image/jpeg', transparent=True)\n",
    "        filename = \"{}{}_{}_{}_{}.jpg\".format(TILE_FOLDER, bbox[0], bbox[1], bbox[2], bbox[3])\n",
    "        out = open(filename, 'wb')\n",
    "        out.write(img.read())\n",
    "        out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Downloading the shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip if you already have the shapefiles. Will take ~ 1hour.\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.makedirs(DATA_FOLDER)\n",
    "\n",
    "for url in URLS_SHAPEFILES:\n",
    "    filename = url.split('/')[-1]\n",
    "    print(\"Downloading file {}\".format(filename))\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(DATA_FOLDER + filename, 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading shapefile and converting to (GEO)Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def json_serial(obj):\n",
    "    \"\"\"JSON serializer for objects not serializable by default json code\"\"\"\n",
    "\n",
    "    if isinstance(obj, (datetime, date)):\n",
    "        serial = obj.isoformat()\n",
    "        return serial\n",
    "    if isinstance(obj, bytes):\n",
    "        return {'__class__': 'bytes',\n",
    "                '__value__': list(obj)}\n",
    "    raise TypeError (\"Type %s not serializable\" % type(obj))\n",
    "\n",
    "reader = shapefile.Reader(DATA_FOLDER + 'Wegvakken.shp')\n",
    "fields = reader.fields[1:]\n",
    "field_names = [field[0] for field in fields]\n",
    "\n",
    "buffer = []\n",
    "for sr in reader.shapeRecords()[:500000]:\n",
    "    atr = dict(zip(field_names, sr.record))\n",
    "    geom = sr.shape.__geo_interface__\n",
    "    buffer.append(dict(type=\"Feature\", geometry=geom, properties=atr)) \n",
    "\n",
    "\n",
    "json_file = open(json_filename, \"w\")\n",
    "json_file.write(json.dumps({\"type\": \"FeatureCollection\", \"features\": buffer}, indent=2, default=json_serial) + \"\\n\")\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Declaring some variables and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_roadtype = {\n",
    "    \"G\": 'Gemeente',\n",
    "    \"R\": 'Rijk',\n",
    "    \"P\": 'Provincie',\n",
    "    \"W\": 'Waterschap',\n",
    "    'T': 'Andere wegbeheerder',\n",
    "    '' : 'leeg'\n",
    "}\n",
    "\n",
    "dict_roadtype_to_color = {\n",
    "    \"G\": 'red',\n",
    "    \"R\": 'blue',\n",
    "    \"P\": 'green',\n",
    "    \"W\": 'magenta',\n",
    "    'T': 'yellow',\n",
    "    '' : 'leeg'\n",
    "}\n",
    "\n",
    "FEATURES_KEY = 'features'\n",
    "PROPERTIES_KEY = 'properties'\n",
    "GEOMETRY_KEY = 'geometry'\n",
    "COORDINATES_KEY = 'coordinates'\n",
    "WEGSOORT_KEY = 'WEGBEHSRT'\n",
    "\n",
    "MINIMUM_NO_POINTS_PER_TILE = 4\n",
    "POINTS_PER_METER = 0.1\n",
    "\n",
    "INPUT_FOLDER_TILES = './datasets/image_tiles_200/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dict(d1, d2, coordinates, rtype):\n",
    "    coordinate_ll_x = int((coordinates[0] // dx)*dx)\n",
    "    coordinate_ll_y = int((coordinates[1] // dy)*dy)\n",
    "    coordinate_ur_x = int((coordinates[0] // dx)*dx + dx)\n",
    "    coordinate_ur_y = int((coordinates[1] // dy)*dy + dy)\n",
    "    tile = \"{}_{}_{}_{}.jpg\".format(coordinate_ll_x, coordinate_ll_y, coordinate_ur_x, coordinate_ur_y)\n",
    "    \n",
    "    rel_coord_x = (coordinates[0] - coordinate_ll_x) / dx\n",
    "    rel_coord_y = (coordinates[1] - coordinate_ll_y) / dy\n",
    "    value = (rtype, rel_coord_x, rel_coord_y)\n",
    "    d1[tile].append(value)\n",
    "    d2[rtype].add(tile)\n",
    "\n",
    "def coord_is_in_bb(coord, bb):\n",
    "    x_min = bb[0]\n",
    "    y_min = bb[1]\n",
    "    x_max = bb[2]\n",
    "    y_max = bb[3]\n",
    "    return coord[0] > x_min and coord[0] < x_max and coord[1] > y_min and coord[1] < y_max\n",
    "\n",
    "def retrieve_roadtype(elem):\n",
    "    return elem[PROPERTIES_KEY][WEGSOORT_KEY]\n",
    "   \n",
    "def retrieve_coordinates(elem):\n",
    "    return elem[GEOMETRY_KEY][COORDINATES_KEY]\n",
    "\n",
    "def eucledian_distance(p1, p2):\n",
    "    diff = np.array(p2)-np.array(p1)\n",
    "    return np.linalg.norm(diff)\n",
    "\n",
    "def calculate_intermediate_points(p1, p2, no_points):\n",
    "    dx = (p2[0] - p1[0]) / (no_points + 1)\n",
    "    dy = (p2[1] - p1[1]) / (no_points + 1)\n",
    "    return [[p1[0] + i * dx, p1[1] +  i * dy] for i in range(1, no_points+1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Map contents of shapefile to the tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_wegvakken = json_filename\n",
    "dict_wegvakken = json.load(open(filename_wegvakken))[FEATURES_KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tile_contents = defaultdict(list)\n",
    "d_roadtype_tiles = defaultdict(set)\n",
    "    \n",
    "for elem in dict_wegvakken:\n",
    "    coordinates = retrieve_coordinates(elem)\n",
    "    rtype = retrieve_roadtype(elem)\n",
    "    coordinates_in_bb = [coord for coord in coordinates if coord_is_in_bb(coord, bounding_box)]\n",
    "    if len(coordinates_in_bb)==1:\n",
    "        coord = coordinates_in_bb[0]\n",
    "        add_to_dict(d_tile_contents, d_roadtype_tiles, coord, rtype)\n",
    "    if len(coordinates_in_bb)>1:\n",
    "        add_to_dict(d_tile_contents, d_roadtype_tiles, coordinates_in_bb[0], rtype)\n",
    "        for ii in range(1,len(coordinates_in_bb)):\n",
    "            previous_coord = coordinates_in_bb[ii-1]\n",
    "            coord = coordinates_in_bb[ii]\n",
    "            add_to_dict(d_tile_contents, d_roadtype_tiles, coord, rtype)\n",
    "            \n",
    "            dist = eucledian_distance(previous_coord, coord)\n",
    "            no_intermediate_points = int(dist*POINTS_PER_METER)           \n",
    "            intermediate_coordinates = calculate_intermediate_points(previous_coord, coord, no_intermediate_points)\n",
    "            for intermediate_coord in intermediate_coordinates:\n",
    "                add_to_dict(d_tile_contents, d_roadtype_tiles, intermediate_coord, rtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b. Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 95000\n",
    "y0 = 430000\n",
    "\n",
    "fig, axarr = plt.subplots(nrows=11,ncols=11, figsize=(16,16))\n",
    "\n",
    "for ii in range(0,11):\n",
    "    for jj in range(0,11):\n",
    "        ll_x = x0 + ii*dx\n",
    "        ll_y = y0 + jj*dy\n",
    "        ur_x = ll_x + dx\n",
    "        ur_y = ll_y + dy\n",
    "        tile = \"{}_{}_{}_{}.jpg\".format(ll_x, ll_y, ur_x, ur_y)\n",
    "        filename = INPUT_FOLDER_TILES + tile        \n",
    "        tile_contents = d_tile_contents[tile]\n",
    "        \n",
    "        ax = axarr[10-jj, ii]\n",
    "        image = ndimage.imread(filename)\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(rgb_image)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        for elem in tile_contents:\n",
    "            color = dict_roadtype_to_color[elem[0]]\n",
    "            x = elem[1]*256\n",
    "            y = (1-elem[2])*256\n",
    "            ax.scatter(x,y,c=color,s=10)\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 95400\n",
    "y0 = 432000\n",
    "POINTS_PER_METER = 0\n",
    "\n",
    "fig, axarr = plt.subplots(nrows=11,ncols=11, figsize=(16,16))\n",
    "\n",
    "for ii in range(0,11):\n",
    "    for jj in range(0,11):\n",
    "        ll_x = x0 + ii*dx\n",
    "        ll_y = y0 + jj*dy\n",
    "        ur_x = ll_x + dx\n",
    "        ur_y = ll_y + dy\n",
    "        tile = \"{}_{}_{}_{}.jpg\".format(ll_x, ll_y, ur_x, ur_y)\n",
    "        filename = INPUT_FOLDER_TILES + tile\n",
    "        tile_contents = d_tile_contents[tile]\n",
    "        \n",
    "        ax = axarr[10-jj, ii]\n",
    "        image = ndimage.imread(filename)\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(rgb_image)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        for elem in tile_contents:\n",
    "            color = dict_roadtype_to_color[elem[0]]\n",
    "            x = elem[1]*256\n",
    "            y = (1-elem[2])*256\n",
    "            ax.scatter(x,y,c=color,s=10)\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4c. Some statistics about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} tiles containing roads.\".format(len(d_tile_contents.keys())))\n",
    "\n",
    "for rtype in d_roadtype_tiles.keys():\n",
    "    roadtype = dict_roadtype[rtype]\n",
    "    no_tiles = len(d_roadtype_tiles[rtype])\n",
    "    print(\"Of roadtype {} ({}) there are {} tiles.\".format(rtype, roadtype, no_tiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prepare dataset for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "def onehot_encode_labels(labels):\n",
    "    list_possible_labels = list(np.unique(labels))\n",
    "    encoded_labels = map(lambda x: list_possible_labels.index(x), labels)\n",
    "    return encoded_labels\n",
    "\n",
    "def randomize(dataset, labels1, labels2, labels3):\n",
    "    permutation = np.random.permutation(dataset.shape[0])\n",
    "    print(permutation.shape)\n",
    "    print(dataset.shape)\n",
    "    print(labels1.shape)\n",
    "    print(labels2.shape)\n",
    "    print(labels3.shape)\n",
    "    randomized_dataset = dataset[permutation, :, :, :]\n",
    "    randomized_labels1 = labels1[permutation]\n",
    "    randomized_labels2 = labels2[permutation]\n",
    "    randomized_labels3 = labels3[permutation]\n",
    "    return randomized_dataset, randomized_labels1, randomized_labels2, randomized_labels3\n",
    "\n",
    "def one_hot_encode(np_array, num_unique_labels):\n",
    "    return (np.arange(num_unique_labels) == np_array[:,None]).astype(np.float32)\n",
    "\n",
    "def reformat_data(dataset, labels1, labels2, labels3):\n",
    "    dataset, labels1, labels2, labels3 = randomize(dataset, labels1, labels2, labels3)\n",
    "    num_unique_labels1 = len(np.unique(labels1))\n",
    "    num_unique_labels2 = len(np.unique(labels2))\n",
    "    labels1 = one_hot_encode(labels1, num_unique_labels1)\n",
    "    labels2 = one_hot_encode(labels2, num_unique_labels2)\n",
    "    return dataset, labels1, labels2, labels3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 256\n",
    "image_height = 256\n",
    "image_depth = 3\n",
    "total_no_images = 7500\n",
    "\n",
    "image_files = os.listdir(INPUT_FOLDER_TILES)\n",
    "\n",
    "dataset = np.ndarray(shape=(total_no_images, image_width, image_height, image_depth), dtype=np.float32)\n",
    "labels_roadtype = []\n",
    "labels_roadpresence = np.ndarray(total_no_images, dtype=np.float32)\n",
    "labels_filename = []\n",
    "\n",
    "for counter, image in enumerate(image_files):\n",
    "    filename = INPUT_FOLDER_TILES + image\n",
    "    labels_filename.append(image)\n",
    "    if image in list(d_tile_contents.keys()):\n",
    "        tile_contents = d_tile_contents[image]\n",
    "        roadtypes = sorted(list(set([elem[0] for elem in tile_contents])))\n",
    "        roadtype = \"_\".join(roadtypes)\n",
    "        labels_roadpresence[counter] = 1\n",
    "    else:\n",
    "        roadtype = ''\n",
    "        labels_roadpresence[counter] = 0\n",
    "    labels_roadtype.append(roadtype)\n",
    "\n",
    "    image_data = ndimage.imread(filename).astype(np.float32)\n",
    "    dataset[counter, :, :] = image_data\n",
    "    if counter % 1000 == 0:\n",
    "        print(\"{} images have been loaded.\".format(counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_filename = np.array(labels_filename)\n",
    "labels_roadtype_ohe = np.array(list(onehot_encode_labels(labels_roadtype)))\n",
    "print(\"Randomizing dataset...\")\n",
    "dataset, labels_roadpresence, labels_roadtype_ohe, labels_filename = reformat_data(dataset, labels_roadpresence, labels_roadtype_ohe, labels_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only used a subset to train and test\n",
    "start_train_dataset = 0\n",
    "start_valid_dataset = 120\n",
    "start_test_dataset = 160\n",
    "total_no_images = 200\n",
    "\n",
    "output_pickle_file = './data/sattelite_dataset.pickle'\n",
    "\n",
    "f = open(output_pickle_file, 'wb')\n",
    "save = {\n",
    "'train_dataset': dataset[start_train_dataset:start_valid_dataset,:,:,:],\n",
    "'train_labels_roadtype': labels_roadtype[start_train_dataset:start_valid_dataset],\n",
    "'train_labels_roadpresence': labels_roadpresence[start_train_dataset:start_valid_dataset],\n",
    "'train_labels_filename': labels_filename[start_train_dataset:start_valid_dataset],\n",
    "'valid_dataset': dataset[start_valid_dataset:start_test_dataset,:,:,:],\n",
    "'valid_labels_roadtype': labels_roadtype[start_valid_dataset:start_test_dataset],\n",
    "'valid_labels_roadpresence': labels_roadpresence[start_valid_dataset:start_test_dataset],\n",
    "'valid_labels_filename': labels_filename[start_valid_dataset:start_test_dataset],\n",
    "'test_dataset': dataset[start_test_dataset:total_no_images,:,:,:],\n",
    "'test_labels_roadtype': labels_roadtype[start_test_dataset:total_no_images],\n",
    "'test_labels_roadpresence': labels_roadpresence[start_test_dataset:total_no_images],\n",
    "'test_labels_filename': labels_filename[start_test_dataset:total_no_images]\n",
    "}\n",
    "pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()\n",
    "\n",
    "print(\"\\nsaved dataset to {}\".format(output_pickle_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. The Convolutional neural network part (Using alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from cnn_models.alexnet import * \n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = './data/sattelite_dataset.pickle'\n",
    "f = open(pickle_file, 'rb')\n",
    "save = pickle.load(f)\n",
    "\n",
    "train_dataset = save['train_dataset'].astype(dtype = np.float32)\n",
    "train_labels = save['train_labels_roadpresence'].astype(dtype = np.float32)\n",
    "valid_dataset = save['valid_dataset'].astype(dtype = np.float32)\n",
    "valid_labels = save['valid_labels_roadpresence'].astype(dtype = np.float32)\n",
    "test_dataset = save['test_dataset'].astype(dtype = np.float32)\n",
    "test_labels = save['test_labels_roadpresence'].astype(dtype = np.float32)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.]\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(np.unique(train_labels))\n",
    "print(np.unique(train_labels))\n",
    "image_width = 256\n",
    "image_height = 256\n",
    "image_depth = 3\n",
    "#num_steps = 501\n",
    "#display_step = 10\n",
    "learning_rate = 0.01\n",
    "batch_size = 5\n",
    "#lambda_loss_amount = 0.0015\n",
    "train_accuracies, test_accuracies, valid_accuracies = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 40 samples\n",
      "Epoch 1/20\n",
      "120/120 [==============================] - 35s 290ms/step - loss: 1.3655 - acc: 0.5958 - val_loss: 2.8156 - val_acc: 0.5375\n",
      "Epoch 2/20\n",
      "120/120 [==============================] - 33s 277ms/step - loss: 1.1389 - acc: 0.6458 - val_loss: 1.0198 - val_acc: 0.7000\n",
      "Epoch 3/20\n",
      "120/120 [==============================] - 33s 272ms/step - loss: 0.8084 - acc: 0.7375 - val_loss: 3.2355 - val_acc: 0.5750\n",
      "Epoch 4/20\n",
      "120/120 [==============================] - 32s 266ms/step - loss: 0.7917 - acc: 0.7708 - val_loss: 1.5507 - val_acc: 0.6125\n",
      "Epoch 5/20\n",
      "120/120 [==============================] - 32s 268ms/step - loss: 0.4863 - acc: 0.8208 - val_loss: 1.2451 - val_acc: 0.6375\n",
      "Epoch 6/20\n",
      "120/120 [==============================] - 33s 273ms/step - loss: 0.6936 - acc: 0.7917 - val_loss: 1.5132 - val_acc: 0.7000\n",
      "Epoch 7/20\n",
      "120/120 [==============================] - 33s 272ms/step - loss: 0.5672 - acc: 0.7917 - val_loss: 0.8989 - val_acc: 0.7125\n",
      "Epoch 8/20\n",
      "120/120 [==============================] - 33s 272ms/step - loss: 0.6288 - acc: 0.7750 - val_loss: 0.6984 - val_acc: 0.6875\n",
      "Epoch 9/20\n",
      "120/120 [==============================] - 32s 270ms/step - loss: 0.5796 - acc: 0.7958 - val_loss: 0.9417 - val_acc: 0.6625\n",
      "Epoch 10/20\n",
      "120/120 [==============================] - 37s 307ms/step - loss: 0.3120 - acc: 0.8750 - val_loss: 1.4330 - val_acc: 0.6000\n",
      "Epoch 11/20\n",
      "120/120 [==============================] - 33s 277ms/step - loss: 0.4111 - acc: 0.8333 - val_loss: 1.7576 - val_acc: 0.7000\n",
      "Epoch 12/20\n",
      "120/120 [==============================] - 34s 281ms/step - loss: 0.4383 - acc: 0.8417 - val_loss: 1.2724 - val_acc: 0.6875\n",
      "Epoch 13/20\n",
      "120/120 [==============================] - 34s 286ms/step - loss: 0.3822 - acc: 0.8583 - val_loss: 1.3649 - val_acc: 0.6375\n",
      "Epoch 14/20\n",
      "120/120 [==============================] - 33s 272ms/step - loss: 0.2240 - acc: 0.9042 - val_loss: 1.2720 - val_acc: 0.6875\n",
      "Epoch 15/20\n",
      "120/120 [==============================] - 33s 277ms/step - loss: 0.5109 - acc: 0.8208 - val_loss: 1.4448 - val_acc: 0.6875\n",
      "Epoch 16/20\n",
      "120/120 [==============================] - 34s 284ms/step - loss: 0.2743 - acc: 0.9042 - val_loss: 1.6035 - val_acc: 0.6500\n",
      "Epoch 17/20\n",
      "120/120 [==============================] - 33s 278ms/step - loss: 0.1491 - acc: 0.9458 - val_loss: 1.6208 - val_acc: 0.6250\n",
      "Epoch 18/20\n",
      "120/120 [==============================] - 33s 276ms/step - loss: 0.1668 - acc: 0.9375 - val_loss: 1.5670 - val_acc: 0.6625\n",
      "Epoch 19/20\n",
      "120/120 [==============================] - 34s 280ms/step - loss: 0.1615 - acc: 0.9375 - val_loss: 1.6148 - val_acc: 0.6375\n",
      "Epoch 20/20\n",
      "120/120 [==============================] - 33s 277ms/step - loss: 0.1327 - acc: 0.9333 - val_loss: 1.3504 - val_acc: 0.7000\n"
     ]
    }
   ],
   "source": [
    "model = alexnet(input_shape = (256, 256, 3), labels = 2)\n",
    "#adam = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "sgd = optimizers.SGD(lr=learning_rate, clipvalue=0.5)\n",
    "model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "train_history = model.fit(train_dataset, train_labels, validation_data = (valid_dataset, valid_labels), epochs = 20, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies = train_history.history['acc']\n",
    "valid_accuracies = train_history.history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59583334159106016, 0.64583333767950535, 0.73750000229726231, 0.7708333358168602, 0.82083333159486449, 0.7916666716337204, 0.79166666666666663, 0.77500000471870101, 0.79583332935969031, 0.875, 0.83333333395421505, 0.84166667113701499, 0.85833333308498061, 0.90416666616996133, 0.82083333532015479, 0.90416666492819786, 0.94583333283662796, 0.93749999751647317, 0.9375, 0.93333333233992255]\n",
      "[0.53750001080334187, 0.70000000298023224, 0.57500000856816769, 0.61250001192092896, 0.63750000670552254, 0.70000001043081284, 0.71250000968575478, 0.6875000037252903, 0.66249999776482582, 0.60000001266598701, 0.70000001043081284, 0.6875000111758709, 0.63750000856816769, 0.68750000931322575, 0.6875000111758709, 0.65000001341104507, 0.6250000111758709, 0.66250001266598701, 0.63750001415610313, 0.70000001043081284]\n"
     ]
    }
   ],
   "source": [
    "print(train_accuracies)\n",
    "print(valid_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01346599  0.01685632]\n",
      " [ 0.03972632  0.05100331]\n",
      " [ 0.20077752  0.54945457]\n",
      " ..., \n",
      " [ 1.37272906  1.9181999 ]\n",
      " [ 0.19040987  0.18162739]\n",
      " [ 0.10887548  0.15343776]]\n"
     ]
    }
   ],
   "source": [
    "#Checking gradient values of the loss wrt weights of last but one layer\n",
    "weights = model.trainable_weights # weight tensors\n",
    "grads = K.function([model.layers[0].input],K.gradients(model.layers[-1].output, weights[-2]))\n",
    "gradients = grads([train_dataset])[0]\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -2.37390447   1.67999411]\n",
      " [-17.14144516  15.55389309]\n",
      " [  4.41476631  -4.27379704]\n",
      " [ -8.48066807   5.66027117]\n",
      " [ -9.50331497  10.63446236]\n",
      " [ -4.44882822   4.70606232]\n",
      " [  4.33899879  -5.08822298]\n",
      " [-14.65424728  11.8973608 ]\n",
      " [  5.1304512   -5.59927368]\n",
      " [ -6.53861713   6.17846489]\n",
      " [-12.56668758  11.51068592]\n",
      " [-10.44581509   9.33745003]\n",
      " [  4.18962812  -3.85089421]\n",
      " [  5.23863029  -5.00057077]\n",
      " [-10.94320011   9.30765343]\n",
      " [  0.66226667  -0.90311652]\n",
      " [  3.61726046  -3.82936931]\n",
      " [ -2.51193547   2.19875288]\n",
      " [-11.38376904   8.46391678]\n",
      " [ -6.30064631   6.4727006 ]\n",
      " [  4.43072081  -5.04810667]\n",
      " [ -4.34134865   5.36832523]\n",
      " [ -5.87984943   6.07151985]\n",
      " [-16.50743103  13.09115696]\n",
      " [ -9.50287247   8.3850174 ]\n",
      " [  5.89523602  -6.78514814]\n",
      " [ -3.51252723   4.45802689]\n",
      " [  4.54786444  -4.40970707]\n",
      " [  4.30904388  -4.15886736]\n",
      " [ -6.31152058   5.9279213 ]\n",
      " [-13.47897243   9.80414486]\n",
      " [  9.76384068  -8.56815052]\n",
      " [-13.46442509  12.20893383]\n",
      " [  5.10202456  -4.3042984 ]\n",
      " [  3.71045303  -3.22930121]\n",
      " [-11.17120171  10.33821964]\n",
      " [  4.14548683  -3.68112016]\n",
      " [  4.15929937  -4.22109509]\n",
      " [  4.42323303  -5.01266527]\n",
      " [ -5.73675823   5.74515915]\n",
      " [-14.13278294  12.19643974]\n",
      " [  3.9343648   -3.69582272]\n",
      " [-11.36521816   9.41735935]\n",
      " [  2.73666239  -3.015836  ]\n",
      " [ -8.10830975   7.06680441]\n",
      " [-11.57805252   7.49663448]\n",
      " [ -6.90638304   7.40808058]\n",
      " [ -6.93816376   6.09033966]\n",
      " [  4.2128377   -4.3319931 ]\n",
      " [-19.10182762  15.94738388]\n",
      " [ -8.99841118   8.24174213]\n",
      " [  5.34654045  -5.34098005]\n",
      " [ -4.31363392   4.71984243]\n",
      " [ -1.81286049   2.0313015 ]\n",
      " [  2.73869801  -2.16697955]\n",
      " [ -4.84221697   3.6072619 ]\n",
      " [ -8.32534695   6.66231394]\n",
      " [ -7.95343304   7.51486444]\n",
      " [ -7.45643616   6.90493965]\n",
      " [  4.95612478  -5.49481392]\n",
      " [ -4.33612394   5.21904659]\n",
      " [  4.83292198  -5.14783907]\n",
      " [-12.4748888    9.08588028]\n",
      " [-18.200634    17.62171364]\n",
      " [  3.27459264  -3.1536386 ]\n",
      " [ -2.13519073   1.52333522]\n",
      " [ -1.68948162   1.29590034]\n",
      " [ -7.4374361    8.68639755]\n",
      " [  4.8948617   -4.86761999]\n",
      " [  5.61517048  -6.23685455]\n",
      " [  4.20399857  -4.54822588]\n",
      " [  7.53355885  -7.89153528]\n",
      " [ -7.04880571   2.57647562]\n",
      " [ -7.84234047   8.57786751]\n",
      " [  5.50559139  -5.35720444]\n",
      " [ -6.21714878   4.39145374]\n",
      " [ -6.63319492   6.50372982]\n",
      " [ -5.12719917   5.42988825]\n",
      " [ -5.97948647   5.42939758]\n",
      " [ -0.29377636  -0.12163534]\n",
      " [ -9.2676897    8.02090263]\n",
      " [ -7.10454082   7.8885293 ]\n",
      " [  4.19298744  -4.40293741]\n",
      " [  2.39881897  -2.00518703]\n",
      " [ -7.85518742   6.88233995]\n",
      " [  0.675381    -0.62346911]\n",
      " [  3.68865895  -4.168643  ]\n",
      " [ -6.72909069   5.28542995]\n",
      " [  3.46856403  -3.67396593]\n",
      " [ -9.41102314   9.02465534]\n",
      " [  5.56901121  -5.8020339 ]\n",
      " [ -6.32907581   5.50169563]\n",
      " [-11.72942257   8.77016163]\n",
      " [  4.67408085  -4.87380457]\n",
      " [  0.66416925  -1.03745365]\n",
      " [ -7.1412077    7.10660458]\n",
      " [-11.60736179  10.31554508]\n",
      " [  4.76004744  -4.67160463]\n",
      " [  1.84973907  -3.25629616]\n",
      " [  5.04333687  -5.38919353]\n",
      " [ -7.54380274   7.06264734]\n",
      " [ -3.07043028   2.7220211 ]\n",
      " [  4.82413816  -5.14329767]\n",
      " [  2.77351689  -4.25645018]\n",
      " [  5.7764492   -6.70078087]\n",
      " [ -5.87326288   5.83699131]\n",
      " [  2.52734137  -5.48916197]\n",
      " [  2.25136495  -2.9494648 ]\n",
      " [ -2.61232853   1.75810146]\n",
      " [ -3.92152309   3.32341981]\n",
      " [  3.69619226  -4.37484694]\n",
      " [-11.54878426  10.20795155]\n",
      " [ -2.06348133   1.53224325]\n",
      " [  4.00908041  -4.34254313]\n",
      " [ -1.92566776   2.67685866]\n",
      " [  3.20677733  -2.43247199]\n",
      " [  5.1432972   -5.47378111]\n",
      " [ -9.43506432   9.28632259]\n",
      " [  3.34877539  -3.96457863]\n",
      " [ -3.91053534   4.18833256]]\n"
     ]
    }
   ],
   "source": [
    "#Output values before activation in last layer\n",
    "get_last_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[-2].output])\n",
    "layer_output = get_last_layer_output([train_dataset])[0]\n",
    "print(layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 3s 66ms/step\n",
      "Loss = 0.738846516609\n",
      "Test Accuracy = 0.8\n"
     ]
    }
   ],
   "source": [
    "test_results = model.evaluate(test_dataset, test_labels)\n",
    "print (\"Loss = \" + str(test_results[0]))\n",
    "print (\"Test Accuracy = \" + str(test_results[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
